{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fc4ee3-120e-4b13-a387-a5db05c58347",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce75861-dcd4-4e2d-8f1f-1d555ad26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd516c4-83ae-451d-8422-ca664e60e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Sentinel-2all\"\n",
    "mixed_precision = \"fp16\"\n",
    "seed = 123\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "nb_epochs = 2\n",
    "train_mode = \"tune\"\n",
    "\n",
    "train_args = (dataset, mixed_precision, seed, batch_size, gradient_accumulation_steps, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99453c0a-a3c6-4644-b248-3f27df18bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: fp16, 123, 4, 4, 2, \n",
      "Device: cuda\n",
      "Train samples: 6951 Val. samples: 1738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1738 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1738 [00:06<2:55:05,  6.05s/it]\u001b[A\n",
      "  0%|          | 2/1738 [00:06<1:21:41,  2.82s/it]\u001b[A\n",
      "  0%|          | 3/1738 [00:07<51:46,  1.79s/it]  \u001b[A\n",
      "  0%|          | 4/1738 [00:07<37:51,  1.31s/it]\u001b[A\n",
      "  0%|          | 5/1738 [00:08<30:05,  1.04s/it]\u001b[A\n",
      "  0%|          | 6/1738 [00:08<25:25,  1.14it/s]\u001b[A\n",
      "  0%|          | 7/1738 [00:09<22:45,  1.27it/s]\u001b[A\n",
      "  0%|          | 8/1738 [00:10<20:46,  1.39it/s]\u001b[A\n",
      "  1%|          | 9/1738 [00:10<21:06,  1.37it/s]\u001b[A\n",
      "  1%|          | 10/1738 [00:11<19:50,  1.45it/s]\u001b[A\n",
      "  1%|          | 11/1738 [00:11<18:53,  1.52it/s]\u001b[A\n",
      "  1%|          | 12/1738 [00:12<18:13,  1.58it/s]\u001b[A\n",
      "  1%|          | 13/1738 [00:13<24:19,  1.18it/s]\u001b[A\n",
      "  1%|          | 14/1738 [00:14<21:33,  1.33it/s]\u001b[A\n",
      "  1%|          | 15/1738 [00:15<20:15,  1.42it/s]\u001b[A\n",
      "  1%|          | 16/1738 [00:15<19:02,  1.51it/s]\u001b[A\n",
      "  1%|          | 17/1738 [00:17<26:25,  1.09it/s]\u001b[A\n",
      "  1%|          | 18/1738 [00:17<23:19,  1.23it/s]\u001b[A\n",
      "  1%|          | 19/1738 [00:18<21:17,  1.35it/s]\u001b[A\n",
      "  1%|          | 20/1738 [00:18<19:40,  1.45it/s]\u001b[A\n",
      "  1%|          | 21/1738 [00:20<26:41,  1.07it/s]\u001b[A\n",
      "  1%|▏         | 22/1738 [00:20<23:36,  1.21it/s]\u001b[A\n",
      "  1%|▏         | 23/1738 [00:21<21:29,  1.33it/s]\u001b[A\n",
      "  1%|▏         | 24/1738 [00:22<19:47,  1.44it/s]\u001b[A\n",
      "  1%|▏         | 25/1738 [00:23<27:54,  1.02it/s]\u001b[A\n",
      "  1%|▏         | 26/1738 [00:24<24:28,  1.17it/s]\u001b[A\n",
      "  2%|▏         | 27/1738 [00:24<22:15,  1.28it/s]\u001b[A\n",
      "  2%|▏         | 28/1738 [00:25<20:30,  1.39it/s]\u001b[A\n",
      "  2%|▏         | 29/1738 [00:26<26:39,  1.07it/s]\u001b[A\n",
      "  2%|▏         | 30/1738 [00:27<23:36,  1.21it/s]\u001b[A\n",
      "  2%|▏         | 31/1738 [00:28<21:21,  1.33it/s]\u001b[A\n",
      "  2%|▏         | 32/1738 [00:28<19:53,  1.43it/s]\u001b[A\n",
      "  2%|▏         | 33/1738 [00:29<21:52,  1.30it/s]\u001b[A\n",
      "  2%|▏         | 34/1738 [00:30<20:18,  1.40it/s]\u001b[A\n",
      "  2%|▏         | 35/1738 [00:30<19:17,  1.47it/s]\u001b[A\n",
      "  2%|▏         | 36/1738 [00:31<18:39,  1.52it/s]\u001b[A\n",
      "  2%|▏         | 37/1738 [00:32<24:27,  1.16it/s]\u001b[A\n",
      "  2%|▏         | 38/1738 [00:33<22:01,  1.29it/s]\u001b[A\n",
      "  2%|▏         | 39/1738 [00:33<20:17,  1.40it/s]\u001b[A\n",
      "  2%|▏         | 40/1738 [00:34<19:07,  1.48it/s]\u001b[A\n",
      "  2%|▏         | 41/1738 [00:35<21:27,  1.32it/s]\u001b[A\n",
      "  2%|▏         | 42/1738 [00:35<19:46,  1.43it/s]\u001b[A\n",
      "  2%|▏         | 43/1738 [00:36<18:43,  1.51it/s]\u001b[A\n",
      "  3%|▎         | 44/1738 [00:37<18:00,  1.57it/s]\u001b[A\n",
      "  3%|▎         | 45/1738 [00:38<25:44,  1.10it/s]\u001b[A\n",
      "  3%|▎         | 46/1738 [00:39<22:54,  1.23it/s]\u001b[A\n",
      "  3%|▎         | 47/1738 [00:39<20:47,  1.36it/s]\u001b[A\n",
      "  3%|▎         | 48/1738 [00:40<19:25,  1.45it/s]\u001b[A\n",
      "  3%|▎         | 49/1738 [00:42<29:07,  1.03s/it]\u001b[A\n",
      "  3%|▎         | 50/1738 [00:42<25:20,  1.11it/s]\u001b[A\n",
      "  3%|▎         | 51/1738 [00:43<22:33,  1.25it/s]\u001b[A\n",
      "  3%|▎         | 52/1738 [00:43<20:34,  1.37it/s]\u001b[A\n",
      "  3%|▎         | 53/1738 [00:45<24:55,  1.13it/s]\u001b[A\n",
      "  3%|▎         | 54/1738 [00:45<22:15,  1.26it/s]\u001b[A\n",
      "  3%|▎         | 55/1738 [00:46<20:21,  1.38it/s]\u001b[A\n",
      "  3%|▎         | 56/1738 [00:46<18:59,  1.48it/s]\u001b[A\n",
      "  3%|▎         | 57/1738 [00:47<20:10,  1.39it/s]\u001b[A\n",
      "  3%|▎         | 58/1738 [00:48<18:55,  1.48it/s]\u001b[A\n",
      "  3%|▎         | 59/1738 [00:48<18:10,  1.54it/s]\u001b[A\n",
      "  3%|▎         | 60/1738 [00:49<17:26,  1.60it/s]\u001b[A\n",
      "  4%|▎         | 61/1738 [00:50<22:59,  1.22it/s]\u001b[A\n",
      "  4%|▎         | 62/1738 [00:51<21:09,  1.32it/s]\u001b[A\n",
      "  4%|▎         | 63/1738 [00:51<19:29,  1.43it/s]\u001b[A\n",
      "  4%|▎         | 64/1738 [00:52<18:24,  1.52it/s]\u001b[A\n",
      "  4%|▎         | 65/1738 [00:53<21:47,  1.28it/s]\u001b[A\n",
      "  4%|▍         | 66/1738 [00:54<19:54,  1.40it/s]\u001b[A\n",
      "  4%|▍         | 67/1738 [00:54<18:37,  1.49it/s]\u001b[A\n",
      "  4%|▍         | 68/1738 [00:55<17:48,  1.56it/s]\u001b[A\n",
      "  4%|▍         | 69/1738 [00:56<20:44,  1.34it/s]\u001b[A\n",
      "  4%|▍         | 70/1738 [00:56<19:15,  1.44it/s]\u001b[A\n",
      "  4%|▍         | 71/1738 [00:57<18:17,  1.52it/s]\u001b[A\n",
      "  4%|▍         | 72/1738 [00:57<17:37,  1.58it/s]\u001b[A\n",
      "  4%|▍         | 73/1738 [00:59<23:53,  1.16it/s]\u001b[A\n",
      "  4%|▍         | 74/1738 [00:59<21:23,  1.30it/s]\u001b[A\n",
      "  4%|▍         | 75/1738 [01:00<19:39,  1.41it/s]\u001b[A\n",
      "  4%|▍         | 76/1738 [01:01<18:35,  1.49it/s]\u001b[A\n",
      "  4%|▍         | 77/1738 [01:02<21:21,  1.30it/s]\u001b[A\n",
      "  4%|▍         | 78/1738 [01:02<19:42,  1.40it/s]\u001b[A\n",
      "  5%|▍         | 79/1738 [01:03<18:30,  1.49it/s]\u001b[A\n",
      "  5%|▍         | 80/1738 [01:03<17:46,  1.55it/s]\u001b[A\n",
      "  5%|▍         | 81/1738 [01:05<24:12,  1.14it/s]\u001b[A\n",
      "  5%|▍         | 82/1738 [01:05<21:47,  1.27it/s]\u001b[A\n",
      "  5%|▍         | 83/1738 [01:06<19:51,  1.39it/s]\u001b[A\n",
      "  5%|▍         | 84/1738 [01:06<18:37,  1.48it/s]\u001b[A\n",
      "  5%|▍         | 85/1738 [01:08<22:19,  1.23it/s]\u001b[A\n",
      "  5%|▍         | 86/1738 [01:08<20:16,  1.36it/s]\u001b[A\n",
      "  5%|▌         | 87/1738 [01:09<18:58,  1.45it/s]\u001b[A\n",
      "  5%|▌         | 88/1738 [01:09<18:03,  1.52it/s]\u001b[A\n",
      "  5%|▌         | 89/1738 [01:11<24:24,  1.13it/s]\u001b[A\n",
      "  5%|▌         | 90/1738 [01:11<21:43,  1.26it/s]\u001b[A\n",
      "  5%|▌         | 91/1738 [01:12<19:55,  1.38it/s]\u001b[A\n",
      "  5%|▌         | 92/1738 [01:12<18:37,  1.47it/s]\u001b[A\n",
      "  5%|▌         | 93/1738 [01:13<17:42,  1.55it/s]\u001b[A\n",
      "  5%|▌         | 94/1738 [01:14<17:02,  1.61it/s]\u001b[A\n",
      "  5%|▌         | 95/1738 [01:14<16:36,  1.65it/s]\u001b[A\n",
      "  6%|▌         | 96/1738 [01:15<16:28,  1.66it/s]\u001b[A\n",
      "  6%|▌         | 97/1738 [01:16<23:00,  1.19it/s]\u001b[A\n",
      "  6%|▌         | 98/1738 [01:17<20:45,  1.32it/s]\u001b[A\n",
      "  6%|▌         | 99/1738 [01:17<19:10,  1.42it/s]\u001b[A\n",
      "  6%|▌         | 100/1738 [01:18<19:28,  1.40it/s]\u001b[A\n",
      "  6%|▌         | 101/1738 [01:19<18:13,  1.50it/s]\u001b[A\n",
      "  6%|▌         | 102/1738 [01:19<17:20,  1.57it/s]\u001b[A\n",
      "  6%|▌         | 103/1738 [01:20<16:48,  1.62it/s]\u001b[A\n",
      "  6%|▌         | 104/1738 [01:21<20:52,  1.30it/s]\u001b[A\n",
      "  6%|▌         | 105/1738 [01:21<19:16,  1.41it/s]\u001b[A\n",
      "  6%|▌         | 106/1738 [01:22<18:09,  1.50it/s]\u001b[A\n",
      "  6%|▌         | 107/1738 [01:22<17:21,  1.57it/s]\u001b[A\n",
      "  6%|▌         | 108/1738 [01:23<18:41,  1.45it/s]\u001b[A\n",
      "  6%|▋         | 109/1738 [01:24<17:52,  1.52it/s]\u001b[A\n",
      "  6%|▋         | 110/1738 [01:24<17:13,  1.58it/s]\u001b[A\n",
      "  6%|▋         | 111/1738 [01:25<19:39,  1.38it/s]\u001b[A\n",
      "  6%|▋         | 112/1738 [01:26<20:13,  1.34it/s]\u001b[A\n",
      "  7%|▋         | 113/1738 [01:27<21:14,  1.27it/s]\u001b[A\n",
      "  7%|▋         | 114/1738 [01:28<19:31,  1.39it/s]\u001b[A\n",
      "  7%|▋         | 115/1738 [01:30<36:50,  1.36s/it]\u001b[A\n",
      "  7%|▋         | 116/1738 [01:31<30:28,  1.13s/it]\u001b[A\n",
      "  7%|▋         | 117/1738 [01:32<25:54,  1.04it/s]\u001b[A\n",
      "  7%|▋         | 118/1738 [01:32<22:54,  1.18it/s]\u001b[A\n",
      "  7%|▋         | 119/1738 [01:33<23:11,  1.16it/s]\u001b[A\n",
      "  7%|▋         | 120/1738 [01:34<20:51,  1.29it/s]\u001b[A\n",
      "  7%|▋         | 121/1738 [01:35<21:18,  1.26it/s]\u001b[A\n",
      "  7%|▋         | 122/1738 [01:35<19:44,  1.36it/s]\u001b[A\n",
      "  7%|▋         | 123/1738 [01:36<19:49,  1.36it/s]\u001b[A\n",
      "  7%|▋         | 124/1738 [01:37<21:47,  1.23it/s]\u001b[A\n",
      "  7%|▋         | 125/1738 [01:38<25:51,  1.04it/s]\u001b[A\n",
      "  7%|▋         | 126/1738 [01:39<22:49,  1.18it/s]\u001b[A\n",
      "  7%|▋         | 127/1738 [01:39<20:37,  1.30it/s]\u001b[A\n",
      "  7%|▋         | 128/1738 [01:40<21:13,  1.26it/s]\u001b[A\n",
      "  7%|▋         | 129/1738 [01:42<26:27,  1.01it/s]\u001b[A\n",
      "  7%|▋         | 130/1738 [01:42<23:14,  1.15it/s]\u001b[A\n",
      "  8%|▊         | 131/1738 [01:43<20:56,  1.28it/s]\u001b[A\n",
      "  8%|▊         | 132/1738 [01:43<19:15,  1.39it/s]\u001b[A\n",
      "  8%|▊         | 133/1738 [01:44<21:23,  1.25it/s]\u001b[A\n",
      "  8%|▊         | 134/1738 [01:45<19:35,  1.37it/s]\u001b[A\n",
      "  8%|▊         | 135/1738 [01:45<18:14,  1.46it/s]\u001b[A\n",
      "  8%|▊         | 136/1738 [01:46<17:25,  1.53it/s]\u001b[A\n",
      "  8%|▊         | 137/1738 [01:47<18:28,  1.44it/s]\u001b[A\n",
      "  8%|▊         | 138/1738 [01:48<23:56,  1.11it/s]\u001b[A\n",
      "  8%|▊         | 139/1738 [01:49<21:18,  1.25it/s]\u001b[A\n",
      "  8%|▊         | 140/1738 [01:49<19:25,  1.37it/s]\u001b[A\n",
      "  8%|▊         | 141/1738 [01:50<18:53,  1.41it/s]\u001b[A\n",
      "  8%|▊         | 142/1738 [01:51<24:11,  1.10it/s]\u001b[A\n",
      "  8%|▊         | 143/1738 [01:52<21:28,  1.24it/s]\u001b[A\n",
      "  8%|▊         | 144/1738 [01:53<19:34,  1.36it/s]\u001b[A\n",
      "  8%|▊         | 145/1738 [01:53<18:13,  1.46it/s]\u001b[A\n",
      "  8%|▊         | 146/1738 [01:55<25:16,  1.05it/s]\u001b[A\n",
      "  8%|▊         | 147/1738 [01:55<22:18,  1.19it/s]\u001b[A\n",
      "  9%|▊         | 148/1738 [01:56<20:03,  1.32it/s]\u001b[A\n",
      "  9%|▊         | 149/1738 [01:56<18:33,  1.43it/s]\u001b[A\n",
      "  9%|▊         | 150/1738 [01:57<21:24,  1.24it/s]\u001b[A\n",
      "  9%|▊         | 151/1738 [01:58<19:27,  1.36it/s]\u001b[A\n",
      "  9%|▊         | 152/1738 [01:59<18:06,  1.46it/s]\u001b[A\n",
      "  9%|▉         | 153/1738 [01:59<19:50,  1.33it/s]\u001b[A\n",
      "  9%|▉         | 154/1738 [02:00<18:20,  1.44it/s]\u001b[A\n",
      "  9%|▉         | 155/1738 [02:01<17:16,  1.53it/s]\u001b[A\n",
      "  9%|▉         | 156/1738 [02:01<16:37,  1.59it/s]\u001b[A\n",
      "  9%|▉         | 157/1738 [02:02<19:36,  1.34it/s]\u001b[A\n",
      "  9%|▉         | 158/1738 [02:03<18:40,  1.41it/s]\u001b[A\n",
      "  9%|▉         | 159/1738 [02:03<17:29,  1.50it/s]\u001b[A\n",
      "  9%|▉         | 160/1738 [02:04<16:48,  1.56it/s]\u001b[A\n",
      "  9%|▉         | 161/1738 [02:05<18:58,  1.38it/s]\u001b[A\n",
      "  9%|▉         | 162/1738 [02:06<25:11,  1.04it/s]\u001b[A\n",
      "  9%|▉         | 163/1738 [02:07<22:05,  1.19it/s]\u001b[A\n",
      "  9%|▉         | 164/1738 [02:08<19:43,  1.33it/s]\u001b[A\n",
      "  9%|▉         | 165/1738 [02:08<18:16,  1.43it/s]\u001b[A\n",
      " 10%|▉         | 166/1738 [02:10<26:25,  1.01s/it]\u001b[A\n",
      " 10%|▉         | 167/1738 [02:10<22:54,  1.14it/s]\u001b[A\n",
      " 10%|▉         | 168/1738 [02:11<20:29,  1.28it/s]\u001b[A\n",
      " 10%|▉         | 169/1738 [02:11<18:42,  1.40it/s]\u001b[A\n",
      " 10%|▉         | 170/1738 [02:13<21:15,  1.23it/s]\u001b[A\n",
      " 10%|▉         | 171/1738 [02:13<19:18,  1.35it/s]\u001b[A\n",
      " 10%|▉         | 172/1738 [02:14<18:06,  1.44it/s]\u001b[A\n",
      " 10%|▉         | 173/1738 [02:14<17:07,  1.52it/s]\u001b[A\n",
      " 10%|█         | 174/1738 [02:15<16:25,  1.59it/s]\u001b[A\n",
      " 10%|█         | 175/1738 [02:15<15:57,  1.63it/s]\u001b[A\n",
      " 10%|█         | 176/1738 [02:16<15:43,  1.66it/s]\u001b[A\n",
      " 10%|█         | 177/1738 [02:17<15:21,  1.69it/s]\u001b[A\n",
      " 10%|█         | 178/1738 [02:17<15:30,  1.68it/s]\u001b[A\n",
      " 10%|█         | 179/1738 [02:18<15:25,  1.68it/s]\u001b[A\n",
      " 10%|█         | 180/1738 [02:19<19:14,  1.35it/s]\u001b[A\n",
      " 10%|█         | 181/1738 [02:19<17:55,  1.45it/s]\u001b[A\n",
      " 10%|█         | 182/1738 [02:20<19:04,  1.36it/s]\u001b[A\n",
      " 11%|█         | 183/1738 [02:21<17:50,  1.45it/s]\u001b[A\n",
      " 11%|█         | 184/1738 [02:22<23:12,  1.12it/s]\u001b[A\n",
      " 11%|█         | 185/1738 [02:23<20:43,  1.25it/s]\u001b[A\n",
      " 11%|█         | 186/1738 [02:23<18:55,  1.37it/s]\u001b[A\n",
      " 11%|█         | 187/1738 [02:24<17:39,  1.46it/s]\u001b[A\n",
      " 11%|█         | 188/1738 [02:25<21:18,  1.21it/s]\u001b[A\n",
      " 11%|█         | 189/1738 [02:26<19:18,  1.34it/s]\u001b[A\n",
      " 11%|█         | 190/1738 [02:27<23:42,  1.09it/s]\u001b[A\n",
      " 11%|█         | 191/1738 [02:28<20:58,  1.23it/s]\u001b[A\n",
      " 11%|█         | 192/1738 [02:29<22:58,  1.12it/s]\u001b[A\n",
      " 11%|█         | 193/1738 [02:29<20:28,  1.26it/s]\u001b[A\n",
      " 11%|█         | 194/1738 [02:30<18:51,  1.36it/s]\u001b[A\n",
      " 11%|█         | 195/1738 [02:30<17:34,  1.46it/s]\u001b[A\n",
      " 11%|█▏        | 196/1738 [02:32<23:47,  1.08it/s]\u001b[A\n",
      " 11%|█▏        | 197/1738 [02:32<21:00,  1.22it/s]\u001b[A\n",
      " 11%|█▏        | 198/1738 [02:33<19:04,  1.35it/s]\u001b[A\n",
      " 11%|█▏        | 199/1738 [02:34<17:47,  1.44it/s]\u001b[A\n",
      " 12%|█▏        | 200/1738 [02:35<20:53,  1.23it/s]\u001b[A\n",
      " 12%|█▏        | 201/1738 [02:35<18:59,  1.35it/s]\u001b[A\n",
      " 12%|█▏        | 202/1738 [02:36<17:38,  1.45it/s]\u001b[A\n",
      " 12%|█▏        | 203/1738 [02:36<16:46,  1.53it/s]\u001b[A\n",
      " 12%|█▏        | 204/1738 [02:37<19:18,  1.32it/s]\u001b[A\n",
      " 12%|█▏        | 205/1738 [02:38<17:57,  1.42it/s]\u001b[A\n",
      " 12%|█▏        | 206/1738 [02:38<16:59,  1.50it/s]\u001b[A\n",
      " 12%|█▏        | 207/1738 [02:39<16:15,  1.57it/s]\u001b[A\n",
      " 12%|█▏        | 208/1738 [02:40<17:42,  1.44it/s]\u001b[A\n",
      " 12%|█▏        | 209/1738 [02:40<16:44,  1.52it/s]\u001b[A\n",
      " 12%|█▏        | 210/1738 [02:41<18:04,  1.41it/s]\u001b[A\n",
      " 12%|█▏        | 211/1738 [02:42<17:07,  1.49it/s]\u001b[A\n",
      " 12%|█▏        | 212/1738 [02:43<23:09,  1.10it/s]\u001b[A\n",
      " 12%|█▏        | 213/1738 [02:44<20:30,  1.24it/s]\u001b[A\n",
      " 12%|█▏        | 214/1738 [02:45<20:29,  1.24it/s]\u001b[A\n",
      " 12%|█▏        | 215/1738 [02:45<18:51,  1.35it/s]\u001b[A\n",
      " 12%|█▏        | 216/1738 [02:47<23:25,  1.08it/s]\u001b[A\n",
      " 12%|█▏        | 217/1738 [02:47<20:48,  1.22it/s]\u001b[A\n",
      " 13%|█▎        | 218/1738 [02:48<20:13,  1.25it/s]\u001b[A\n",
      " 13%|█▎        | 219/1738 [02:49<18:25,  1.37it/s]\u001b[A\n",
      " 13%|█▎        | 220/1738 [02:50<19:35,  1.29it/s]\u001b[A\n",
      "  0%|          | 0/2 [02:50<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: /notebooks/notebooks/training.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
       "=============================================================\n",
       "    69    740.7 MiB    740.7 MiB           1   def training_loop(dataset: str,\n",
       "    70                                                           mixed_precision: str=\"fp16\",\n",
       "    71                                                           seed: int=123,\n",
       "    72                                                           batch_size: int=16,\n",
       "    73                                                           gradient_accumulation_steps: int=4,\n",
       "    74                                                           nb_epochs=2,\n",
       "    75                                                           train_mode: str=\"\"\n",
       "    76                                             ):\n",
       "    77                                             \"\"\"Main Training and Evaluation Loop to be called by accelerator.notebook_launcher().\"\"\"\n",
       "    78    740.7 MiB      0.0 MiB           4       print(f\"Args: {mixed_precision}, {seed}, {batch_size}, \"\n",
       "    79    740.7 MiB      0.0 MiB           3             f\"{gradient_accumulation_steps}, {nb_epochs}, {train_mode}\")\n",
       "    80                                             \n",
       "    81                                         \n",
       "    82                                             # Metadata\n",
       "    83    740.7 MiB      0.0 MiB           1       metadata_file = \"/notebooks/data/metadata_parquet/features_metadata_slim.parquet\"\n",
       "    84    953.5 MiB    212.7 MiB           1       metadata_df = pd.read_parquet(metadata_file)\n",
       "    85    960.4 MiB      7.0 MiB           1       chip_ids = metadata_df[metadata_df.split == \"train\"].chip_id.unique().tolist()\n",
       "    86                                         \n",
       "    87    960.4 MiB      0.0 MiB           1       artifacts_dir = \"/notebooks/artifacts\"\n",
       "    88    960.4 MiB      0.0 MiB           1       model_name = \"UTAE\"\n",
       "    89    960.4 MiB      0.0 MiB           1       date = \"20230118\"\n",
       "    90    960.4 MiB      0.0 MiB           1       pretrained_weights_path = artifacts_dir + \"/pretrained_utae/f1model.pth.tar\"  # for fine tuning\n",
       "    91                                         \n",
       "    92    960.4 MiB      0.0 MiB           1       saved_state_path = artifacts_dir + \"/20230112_UTAE_S2_B32_E20.pt\"  # for resuming training\n",
       "    93    960.4 MiB      0.0 MiB           2       save_path = artifacts_dir + (f\"/{date}_{model_name}_{dataset}_B\"\n",
       "    94    960.4 MiB      0.0 MiB           1           f\"{batch_size * gradient_accumulation_steps}.pt\")\n",
       "    95    960.4 MiB      0.0 MiB           1       best_model_path = save_path[:-3] + \"_BEST.pt\"\n",
       "    96                                         \n",
       "    97                                             # Set random seed\n",
       "    98    960.4 MiB      0.0 MiB           1       set_seed(seed)\n",
       "    99                                             \n",
       "   100    960.4 MiB      0.0 MiB           1       if torch.cuda.is_available():\n",
       "   101    960.4 MiB      0.0 MiB           1           device = torch.device('cuda')\n",
       "   102                                             else:\n",
       "   103                                                 device = torch.device('cpu')\n",
       "   104    960.4 MiB      0.0 MiB           1       print(f\"Device: {device}\")\n",
       "   105                                             \n",
       "   106                                             # Initialize Accelerator\n",
       "   107                                             # accelerator = Accelerator(mixed_precision=mixed_precision,\n",
       "   108                                                 # gradient_accumulation_steps=gradient_accumulation_steps)\n",
       "   109                                         \n",
       "   110                                             # Build DataLoaders\n",
       "   111    960.4 MiB      0.0 MiB           1       train_dataloader, eval_dataloader = get_dataloaders(chip_ids, dataset, batch_size=batch_size)\n",
       "   112                                         \n",
       "   113                                             # Assign model inputs based on dataset\n",
       "   114    960.4 MiB      0.0 MiB           1       if dataset == \"Sentinel-1A\":\n",
       "   115                                                 input_nc = 2\n",
       "   116                                                 n_tsamples = 6\n",
       "   117    960.4 MiB      0.0 MiB           1       elif dataset == \"Sentinel-1D\":\n",
       "   118                                                 input_nc = 2\n",
       "   119                                                 n_tsamples = 6\n",
       "   120    960.4 MiB      0.0 MiB           1       elif dataset == \"Sentinel-2all\":\n",
       "   121    960.4 MiB      0.0 MiB           1           input_nc = 10\n",
       "   122    960.4 MiB      0.0 MiB           1           n_tsamples = 5\n",
       "   123                                             else:\n",
       "   124                                                 return\n",
       "   125                                         \n",
       "   126                                             # Create model\n",
       "   127    960.4 MiB      0.0 MiB           1       if train_mode == \"tune\":\n",
       "   128                                                 # with init_empty_weights():\n",
       "   129                                                     # model = UTAE(10, out_conv=[32, 20])  # Initialize the original model & load pre-trained weights\n",
       "   130                                                 model = UTAE(10, out_conv=[32, 20])  # Initialize the original model & load pre-trained weights\n",
       "   131                                                 saved_dict = torch.load(pretrained_weights_path)\n",
       "   132                                                 model.load_state_dict(saved_dict[\"state_dict\"])\n",
       "   133                                                 model.out_conv = ConvBlock([32, 32, 1], padding_mode=\"reflect\")  # Modify the last layer\n",
       "   134                                             else:\n",
       "   135    964.3 MiB      3.9 MiB           1           model = UTAE(input_nc)  # modify output layer to predict AGBM\n",
       "   136    964.3 MiB      0.0 MiB           1           if train_mode == \"resume\":\n",
       "   137                                                     state_dict = torch.load(saved_state_path)  # , map_location=accelerator.device)\n",
       "   138                                                     model.load_state_dict(state_dict)\n",
       "   139                                             \n",
       "   140   2441.4 MiB   1477.1 MiB           1       model = model.to(device)\n",
       "   141   2441.4 MiB      0.0 MiB           1       loss_function = nn.MSELoss(reduction='mean').to(device)  # Loss function\n",
       "   142   2441.4 MiB      0.0 MiB           1       optimizer = torch.optim.Adam(model.parameters(), lr=0.02)  # Optimizer\n",
       "   143                                             \n",
       "   144                                             # Prepare everything to use accelerator\n",
       "   145                                             # Maintain order while unpacking\n",
       "   146                                             # model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model,\n",
       "   147                                             #                                                            optimizer,\n",
       "   148                                             #                                                            train_dataloader,\n",
       "   149                                             #                                                            eval_dataloader)\n",
       "   150                                         \n",
       "   151                                             # Training loop\n",
       "   152   2441.4 MiB      0.0 MiB           1       min_valid_metric = np.inf\n",
       "   153   2441.4 MiB      0.0 MiB           1       for i in tqdm(range(nb_epochs)):\n",
       "   154                                                 # accelerator.print(f\"Epoch {i+1}\")\n",
       "   155   2441.4 MiB      0.0 MiB           1           print(f\"Epoch {i+1}\")\n",
       "   156   2441.4 MiB      0.0 MiB           1           epoch_start = time()\n",
       "   157                                                 # accelerator.print(f\"Training\")\n",
       "   158   2441.4 MiB      0.0 MiB           1           print(f\"Training\")\n",
       "   159   3601.7 MiB     -8.5 MiB         221           for batch in tqdm(train_dataloader):\n",
       "   160                                                     # with accelerator.accumulate(model):\n",
       "   161   3552.0 MiB -10965.6 MiB         221               inputs, targets, _ = batch\n",
       "   162   3601.4 MiB  11956.4 MiB         221               outputs = model(inputs.to(device))\n",
       "   163   3601.4 MiB      0.9 MiB         221               loss = loss_function(outputs, targets.to(device))\n",
       "   164                                                     # accelerator.backward(loss)\n",
       "   165   3603.0 MiB     78.7 MiB         221               loss.backward()\n",
       "   166   3601.4 MiB     -3.9 MiB         220               optimizer.step()\n",
       "   167   3601.4 MiB    -10.6 MiB         220               optimizer.zero_grad()\n",
       "   168                                         \n",
       "   169                                                 epoch_end = time()\n",
       "   170                                                 # accelerator.print(f\"  Training time: {epoch_end - epoch_start}\")\n",
       "   171                                                 print(f\"  Training time: {epoch_end - epoch_start}\")\n",
       "   172                                                 \n",
       "   173                                                 # Save Model State Dict after each epoch in order to continue training later\n",
       "   174                                                 # unwrap_model = accelerator.unwrap_model(model)  # Unwrap the Accelerator model\n",
       "   175                                                 train_model_path = save_path[:-3] + f\"_E{i+1}.pt\"\n",
       "   176                                                 # accelerator.save(unwrap_model.state_dict(), train_model_path)\n",
       "   177                                                 # accelerator.print(f\"  Model file path: {train_model_path}\")\n",
       "   178                                                 torch.save(model.state_dict(), train_model_path)\n",
       "   179                                                 print(f\"  Model file path: {train_model_path}\")\n",
       "   180                                              \n",
       "   181                                                 # Validation Loop\n",
       "   182                                                 val_loss = 0.0\n",
       "   183                                                 num_elements = 0\n",
       "   184                                                 # accelerator.print(f\"Validation\")\n",
       "   185                                                 print(f\"Validation\")\n",
       "   186                                                 for batch in tqdm(eval_dataloader):\n",
       "   187                                                     inputs, targets, _ = batch\n",
       "   188                                                     with torch.no_grad():\n",
       "   189                                                         predictions = model(inputs.to(device))\n",
       "   190                                                     # Gather all predictions and targets\n",
       "   191                                                     # all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n",
       "   192                                                     # num_elements += all_predictions.shape[0]\n",
       "   193                                                     # val_loss += loss_function(all_predictions, all_targets).item()\n",
       "   194                                                     val_loss += loss_function(predictions, targets.to(device)).item()\n",
       "   195                                         \n",
       "   196                                                 val_loss /= len(eval_dataloader)\n",
       "   197                                                 val_rmse = np.round(np.sqrt(val_loss), 5)\n",
       "   198                                                 # accelerator.print(f\"  Validation RMSE: {val_rmse:>8f}\")\n",
       "   199                                                 print(f\"  Validation RMSE: {val_rmse:>8f}\")\n",
       "   200                                                 # check validation score, if improved then save model\n",
       "   201                                                 if min_valid_metric > val_rmse:\n",
       "   202                                                     # accelerator.print(f\"  Validation RMSE Decreased({min_valid_metric:.6f}--->{val_rmse:.6f})\")\n",
       "   203                                                     print(f\"  Validation RMSE Decreased({min_valid_metric:.6f}--->{val_rmse:.6f})\")\n",
       "   204                                                     min_valid_metric = val_rmse\n",
       "   205                                         \n",
       "   206                                                     # Saving Model State Dict\n",
       "   207                                                     # unwrap_model = accelerator.unwrap_model(model)  # Unwrap the Accelerator model\n",
       "   208                                                     # accelerator.save(unwrap_model.state_dict(), best_model_path)\n",
       "   209                                                     # accelerator.print(f\"  Best Model file path: {best_model_path}\")\n",
       "   210                                                     torch.save(model.state_dict(), best_model_path)\n",
       "   211                                                     print(f\"  Best Model file path: {best_model_path}\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mprun -f training_loop\n",
    "training_loop(*train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8c4658-9a83-480d-8695-c60126bdc093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
