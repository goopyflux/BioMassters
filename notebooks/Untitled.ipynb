{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fc4ee3-120e-4b13-a387-a5db05c58347",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ce75861-dcd4-4e2d-8f1f-1d555ad26fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from training import training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bd516c4-83ae-451d-8422-ca664e60e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"Sentinel-2all\"\n",
    "mixed_precision = \"fp16\"\n",
    "seed = 123\n",
    "batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "nb_epochs = 2\n",
    "train_mode = \"tune\"\n",
    "\n",
    "train_args = (dataset, mixed_precision, seed, batch_size, gradient_accumulation_steps, nb_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99453c0a-a3c6-4644-b248-3f27df18bc81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args: fp16, 123, 4, 4, 2, \n",
      "Device: cuda\n",
      "Train samples: 6951 Val. samples: 1738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1738 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1738 [00:04<2:04:05,  4.29s/it]\u001b[A\n",
      "  0%|          | 2/1738 [00:05<1:13:50,  2.55s/it]\u001b[A\n",
      "  0%|          | 3/1738 [00:06<58:05,  2.01s/it]  \u001b[A\n",
      "  0%|          | 4/1738 [00:08<50:29,  1.75s/it]\u001b[A\n",
      "  0%|          | 5/1738 [00:09<46:19,  1.60s/it]\u001b[A\n",
      "  0%|          | 6/1738 [00:11<43:43,  1.51s/it]\u001b[A\n",
      "  0%|          | 7/1738 [00:12<42:09,  1.46s/it]\u001b[A\n",
      "  0%|          | 8/1738 [00:13<41:03,  1.42s/it]\u001b[A\n",
      "  1%|          | 9/1738 [00:15<40:19,  1.40s/it]\u001b[A\n",
      "  1%|          | 10/1738 [00:16<39:47,  1.38s/it]\u001b[A\n",
      "  1%|          | 11/1738 [00:17<39:29,  1.37s/it]\u001b[A\n",
      "  1%|          | 12/1738 [00:19<39:15,  1.36s/it]\u001b[A\n",
      "  1%|          | 13/1738 [00:20<39:03,  1.36s/it]\u001b[A\n",
      "  1%|          | 14/1738 [00:21<38:07,  1.33s/it]\u001b[A\n",
      "  1%|          | 15/1738 [00:23<38:38,  1.35s/it]\u001b[A\n",
      "  1%|          | 16/1738 [00:24<38:40,  1.35s/it]\u001b[A\n",
      "  1%|          | 17/1738 [00:25<38:43,  1.35s/it]\u001b[A\n",
      "  1%|          | 18/1738 [00:27<38:44,  1.35s/it]\u001b[A\n",
      "  1%|          | 19/1738 [00:28<38:41,  1.35s/it]\u001b[A\n",
      "  1%|          | 20/1738 [00:29<38:44,  1.35s/it]\u001b[A\n",
      "  1%|          | 21/1738 [00:31<38:42,  1.35s/it]\u001b[A\n",
      "  1%|▏         | 22/1738 [00:32<38:45,  1.36s/it]\u001b[A\n",
      "  1%|▏         | 23/1738 [00:33<38:44,  1.36s/it]\u001b[A\n",
      "  1%|▏         | 24/1738 [00:35<38:29,  1.35s/it]\u001b[A\n",
      "  1%|▏         | 25/1738 [00:36<38:30,  1.35s/it]\u001b[A\n",
      "  1%|▏         | 26/1738 [00:37<38:39,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 27/1738 [00:39<38:40,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 28/1738 [00:40<38:47,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 29/1738 [00:42<38:50,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 30/1738 [00:43<38:45,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 31/1738 [00:44<38:39,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 32/1738 [00:46<38:35,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 33/1738 [00:47<38:32,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 34/1738 [00:48<38:29,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 35/1738 [00:50<38:23,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 36/1738 [00:51<38:24,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 37/1738 [00:52<38:20,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 38/1738 [00:54<38:18,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 39/1738 [00:55<38:14,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 40/1738 [00:56<38:11,  1.35s/it]\u001b[A\n",
      "  2%|▏         | 41/1738 [00:58<38:20,  1.36s/it]\u001b[A\n",
      "  2%|▏         | 42/1738 [00:59<37:42,  1.33s/it]\u001b[A\n",
      "  2%|▏         | 43/1738 [01:00<37:48,  1.34s/it]\u001b[A\n",
      "  3%|▎         | 44/1738 [01:02<37:53,  1.34s/it]\u001b[A\n",
      "  3%|▎         | 45/1738 [01:03<37:58,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 46/1738 [01:05<38:01,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 47/1738 [01:06<38:02,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 48/1738 [01:07<38:01,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 49/1738 [01:09<38:00,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 50/1738 [01:10<37:58,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 51/1738 [01:11<37:59,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 52/1738 [01:13<38:00,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 53/1738 [01:14<37:57,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 54/1738 [01:15<37:56,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 55/1738 [01:17<37:53,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 56/1738 [01:18<37:52,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 57/1738 [01:19<37:50,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 58/1738 [01:21<37:52,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 59/1738 [01:22<37:50,  1.35s/it]\u001b[A\n",
      "  3%|▎         | 60/1738 [01:23<37:49,  1.35s/it]\u001b[A\n",
      "  4%|▎         | 61/1738 [01:25<37:46,  1.35s/it]\u001b[A\n",
      "  4%|▎         | 62/1738 [01:26<38:04,  1.36s/it]\u001b[A\n",
      "  4%|▎         | 63/1738 [01:28<37:56,  1.36s/it]\u001b[A\n",
      "  4%|▎         | 64/1738 [01:29<37:53,  1.36s/it]\u001b[A\n",
      "  4%|▎         | 65/1738 [01:30<37:51,  1.36s/it]\u001b[A\n",
      "  4%|▍         | 66/1738 [01:32<37:44,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 67/1738 [01:33<37:40,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 68/1738 [01:34<37:38,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 69/1738 [01:36<37:39,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 70/1738 [01:37<37:39,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 71/1738 [01:38<37:42,  1.36s/it]\u001b[A\n",
      "  4%|▍         | 72/1738 [01:40<37:38,  1.36s/it]\u001b[A\n",
      "  4%|▍         | 73/1738 [01:41<37:32,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 74/1738 [01:42<37:35,  1.36s/it]\u001b[A\n",
      "  4%|▍         | 75/1738 [01:44<37:34,  1.36s/it]\u001b[A\n",
      "  4%|▍         | 76/1738 [01:45<37:31,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 77/1738 [01:46<37:29,  1.35s/it]\u001b[A\n",
      "  4%|▍         | 78/1738 [01:48<37:29,  1.36s/it]\u001b[A\n",
      "  5%|▍         | 79/1738 [01:49<37:29,  1.36s/it]\u001b[A\n",
      "  5%|▍         | 80/1738 [01:51<37:43,  1.36s/it]\u001b[A\n",
      "  5%|▍         | 81/1738 [01:52<37:34,  1.36s/it]\u001b[A\n",
      "  5%|▍         | 82/1738 [01:53<37:28,  1.36s/it]\u001b[A\n",
      "  5%|▍         | 83/1738 [01:55<37:22,  1.35s/it]\u001b[A\n",
      "  5%|▍         | 84/1738 [01:56<37:17,  1.35s/it]\u001b[A\n",
      "  5%|▍         | 85/1738 [01:57<37:10,  1.35s/it]\u001b[A\n",
      "  5%|▍         | 86/1738 [01:59<37:18,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 87/1738 [02:00<37:15,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 88/1738 [02:01<37:11,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 89/1738 [02:03<37:12,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 90/1738 [02:04<37:08,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 91/1738 [02:05<37:11,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 92/1738 [02:07<37:06,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 93/1738 [02:08<37:08,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 94/1738 [02:10<37:06,  1.35s/it]\u001b[A\n",
      "  5%|▌         | 95/1738 [02:11<37:12,  1.36s/it]\u001b[A\n",
      "  6%|▌         | 96/1738 [02:12<37:09,  1.36s/it]\u001b[A\n",
      "  6%|▌         | 97/1738 [02:14<37:05,  1.36s/it]\u001b[A\n",
      "  6%|▌         | 98/1738 [02:15<37:02,  1.36s/it]\u001b[A\n",
      "  6%|▌         | 99/1738 [02:16<36:59,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 100/1738 [02:18<37:00,  1.36s/it]\u001b[A\n",
      "  6%|▌         | 101/1738 [02:19<36:53,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 102/1738 [02:20<36:50,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 103/1738 [02:22<36:47,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 104/1738 [02:23<36:43,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 105/1738 [02:24<36:42,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 106/1738 [02:26<36:39,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 107/1738 [02:27<36:38,  1.35s/it]\u001b[A\n",
      "  6%|▌         | 108/1738 [02:28<36:44,  1.35s/it]\u001b[A\n",
      "  6%|▋         | 109/1738 [02:30<36:41,  1.35s/it]\u001b[A\n",
      "  6%|▋         | 110/1738 [02:31<36:44,  1.35s/it]\u001b[A\n",
      "  6%|▋         | 111/1738 [02:33<36:44,  1.36s/it]\u001b[A\n",
      "  6%|▋         | 112/1738 [02:34<36:40,  1.35s/it]\u001b[A\n",
      "  7%|▋         | 113/1738 [02:35<36:38,  1.35s/it]\u001b[A\n",
      "  7%|▋         | 114/1738 [02:37<36:35,  1.35s/it]\u001b[A\n",
      "  7%|▋         | 115/1738 [02:38<36:34,  1.35s/it]\u001b[A\n",
      "  7%|▋         | 116/1738 [02:39<36:34,  1.35s/it]\u001b[A\n",
      "  7%|▋         | 117/1738 [02:41<36:41,  1.36s/it]\u001b[A\n",
      "  7%|▋         | 118/1738 [02:42<36:35,  1.36s/it]\u001b[A\n",
      "  7%|▋         | 119/1738 [02:45<37:28,  1.39s/it]\u001b[A\n",
      "  0%|          | 0/2 [02:45<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: /notebooks/notebooks/training.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
       "=============================================================\n",
       "    69    739.3 MiB    739.3 MiB           1   def training_loop(dataset: str,\n",
       "    70                                                           mixed_precision: str=\"fp16\",\n",
       "    71                                                           seed: int=123,\n",
       "    72                                                           batch_size: int=16,\n",
       "    73                                                           gradient_accumulation_steps: int=4,\n",
       "    74                                                           nb_epochs=2,\n",
       "    75                                                           train_mode: str=\"\"\n",
       "    76                                             ):\n",
       "    77                                             \"\"\"Main Training and Evaluation Loop to be called by accelerator.notebook_launcher().\"\"\"\n",
       "    78    739.5 MiB      0.2 MiB           4       print(f\"Args: {mixed_precision}, {seed}, {batch_size}, \"\n",
       "    79    739.3 MiB      0.0 MiB           3             f\"{gradient_accumulation_steps}, {nb_epochs}, {train_mode}\")\n",
       "    80                                             \n",
       "    81                                         \n",
       "    82                                             # Metadata\n",
       "    83    739.5 MiB      0.0 MiB           1       metadata_file = \"/notebooks/data/metadata_parquet/features_metadata_slim.parquet\"\n",
       "    84    969.5 MiB    230.0 MiB           1       metadata_df = pd.read_parquet(metadata_file)\n",
       "    85    976.5 MiB      7.0 MiB           1       chip_ids = metadata_df[metadata_df.split == \"train\"].chip_id.unique().tolist()\n",
       "    86                                         \n",
       "    87    976.5 MiB      0.0 MiB           1       artifacts_dir = \"/notebooks/artifacts\"\n",
       "    88    976.5 MiB      0.0 MiB           1       model_name = \"UTAE\"\n",
       "    89    976.5 MiB      0.0 MiB           1       date = \"20230118\"\n",
       "    90    976.5 MiB      0.0 MiB           1       pretrained_weights_path = artifacts_dir + \"/pretrained_utae/f1model.pth.tar\"  # for fine tuning\n",
       "    91                                         \n",
       "    92    976.5 MiB      0.0 MiB           1       saved_state_path = artifacts_dir + \"/20230112_UTAE_S2_B32_E20.pt\"  # for resuming training\n",
       "    93    976.5 MiB      0.0 MiB           2       save_path = artifacts_dir + (f\"/{date}_{model_name}_{dataset}_B\"\n",
       "    94    976.5 MiB      0.0 MiB           1           f\"{batch_size * gradient_accumulation_steps}.pt\")\n",
       "    95    976.5 MiB      0.0 MiB           1       best_model_path = save_path[:-3] + \"_BEST.pt\"\n",
       "    96                                         \n",
       "    97                                             # Set random seed\n",
       "    98    976.5 MiB      0.0 MiB           1       set_seed(seed)\n",
       "    99                                             \n",
       "   100    976.5 MiB      0.0 MiB           1       if torch.cuda.is_available():\n",
       "   101    976.5 MiB      0.0 MiB           1           device = torch.device('cuda')\n",
       "   102                                             else:\n",
       "   103                                                 device = torch.device('cpu')\n",
       "   104    976.5 MiB      0.0 MiB           1       print(f\"Device: {device}\")\n",
       "   105                                             \n",
       "   106                                             # Initialize Accelerator\n",
       "   107                                             # accelerator = Accelerator(mixed_precision=mixed_precision,\n",
       "   108                                                 # gradient_accumulation_steps=gradient_accumulation_steps)\n",
       "   109                                         \n",
       "   110                                             # Build DataLoaders\n",
       "   111    976.5 MiB      0.0 MiB           1       train_dataloader, eval_dataloader = get_dataloaders(chip_ids, dataset, batch_size=batch_size)\n",
       "   112                                         \n",
       "   113                                             # Assign model inputs based on dataset\n",
       "   114    976.5 MiB      0.0 MiB           1       if dataset == \"Sentinel-1A\":\n",
       "   115                                                 input_nc = 2\n",
       "   116                                                 n_tsamples = 6\n",
       "   117    976.5 MiB      0.0 MiB           1       elif dataset == \"Sentinel-1D\":\n",
       "   118                                                 input_nc = 2\n",
       "   119                                                 n_tsamples = 6\n",
       "   120    976.5 MiB      0.0 MiB           1       elif dataset == \"Sentinel-2all\":\n",
       "   121    976.5 MiB      0.0 MiB           1           input_nc = 10\n",
       "   122    976.5 MiB      0.0 MiB           1           n_tsamples = 5\n",
       "   123                                             else:\n",
       "   124                                                 return\n",
       "   125                                         \n",
       "   126                                             # Create model\n",
       "   127    976.5 MiB      0.0 MiB           1       if train_mode == \"tune\":\n",
       "   128                                                 # with init_empty_weights():\n",
       "   129                                                     # model = UTAE(10, out_conv=[32, 20])  # Initialize the original model & load pre-trained weights\n",
       "   130                                                 model = UTAE(10, out_conv=[32, 20])  # Initialize the original model & load pre-trained weights\n",
       "   131                                                 saved_dict = torch.load(pretrained_weights_path)\n",
       "   132                                                 model.load_state_dict(saved_dict[\"state_dict\"])\n",
       "   133                                                 model.out_conv = ConvBlock([32, 32, 1], padding_mode=\"reflect\")  # Modify the last layer\n",
       "   134                                             else:\n",
       "   135    980.5 MiB      4.0 MiB           1           model = UTAE(input_nc)  # modify output layer to predict AGBM\n",
       "   136    980.5 MiB      0.0 MiB           1           if train_mode == \"resume\":\n",
       "   137                                                     state_dict = torch.load(saved_state_path)  # , map_location=accelerator.device)\n",
       "   138                                                     model.load_state_dict(state_dict)\n",
       "   139                                             \n",
       "   140   2354.8 MiB   1374.3 MiB           1       model = model.to(device)\n",
       "   141   2354.8 MiB      0.0 MiB           1       loss_function = nn.MSELoss(reduction='mean').to(device)  # Loss function\n",
       "   142   2354.8 MiB      0.0 MiB           1       optimizer = torch.optim.Adam(model.parameters(), lr=0.02)  # Optimizer\n",
       "   143                                             \n",
       "   144                                             # Prepare everything to use accelerator\n",
       "   145                                             # Maintain order while unpacking\n",
       "   146                                             # model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model,\n",
       "   147                                             #                                                            optimizer,\n",
       "   148                                             #                                                            train_dataloader,\n",
       "   149                                             #                                                            eval_dataloader)\n",
       "   150                                         \n",
       "   151                                             # Training loop\n",
       "   152   2354.8 MiB      0.0 MiB           1       min_valid_metric = np.inf\n",
       "   153   2357.1 MiB      2.3 MiB           1       for i in tqdm(range(nb_epochs)):\n",
       "   154                                                 # accelerator.print(f\"Epoch {i+1}\")\n",
       "   155   2357.1 MiB      0.0 MiB           1           print(f\"Epoch {i+1}\")\n",
       "   156   2357.1 MiB      0.0 MiB           1           epoch_start = time()\n",
       "   157                                                 # accelerator.print(f\"Training\")\n",
       "   158   2357.1 MiB      0.0 MiB           1           print(f\"Training\")\n",
       "   159   3441.2 MiB  -6068.2 MiB         120           for batch in tqdm(train_dataloader):\n",
       "   160                                                     # with accelerator.accumulate(model):\n",
       "   161   3490.6 MiB   5932.2 MiB         120               inputs = batch[\"image\"].to(device)\n",
       "   162   3490.6 MiB     -0.1 MiB         120               targets = batch[\"target\"].to(device)\n",
       "   163   3491.2 MiB   1016.7 MiB         120               outputs = model(inputs)\n",
       "   164   3491.2 MiB    -67.1 MiB         120               loss = loss_function(outputs, targets)\n",
       "   165                                                     # accelerator.backward(loss)\n",
       "   166   3492.1 MiB      8.9 MiB         120               loss.backward()\n",
       "   167   3492.1 MiB   -171.8 MiB         120               optimizer.step()\n",
       "   168   3492.3 MiB   -176.2 MiB         120               optimizer.zero_grad()\n",
       "   169                                         \n",
       "   170                                                 epoch_end = time()\n",
       "   171                                                 # accelerator.print(f\"  Training time: {epoch_end - epoch_start}\")\n",
       "   172                                                 print(f\"  Training time: {epoch_end - epoch_start}\")\n",
       "   173                                                 \n",
       "   174                                                 # Save Model State Dict after each epoch in order to continue training later\n",
       "   175                                                 # unwrap_model = accelerator.unwrap_model(model)  # Unwrap the Accelerator model\n",
       "   176                                                 train_model_path = save_path[:-3] + f\"_E{i+1}.pt\"\n",
       "   177                                                 # accelerator.save(unwrap_model.state_dict(), train_model_path)\n",
       "   178                                                 # accelerator.print(f\"  Model file path: {train_model_path}\")\n",
       "   179                                                 torch.save(model.state_dict(), train_model_path)\n",
       "   180                                                 print(f\"  Model file path: {train_model_path}\")\n",
       "   181                                              \n",
       "   182                                                 # Validation Loop\n",
       "   183                                                 val_loss = 0.0\n",
       "   184                                                 num_elements = 0\n",
       "   185                                                 # accelerator.print(f\"Validation\")\n",
       "   186                                                 print(f\"Validation\")\n",
       "   187                                                 for batch in tqdm(eval_dataloader):\n",
       "   188                                                     inputs = batch[\"image\"].to(device)\n",
       "   189                                                     targets = batch[\"target\"].to(device)\n",
       "   190                                                     with torch.no_grad():\n",
       "   191                                                         predictions = model(inputs)\n",
       "   192                                                     # Gather all predictions and targets\n",
       "   193                                                     # all_predictions, all_targets = accelerator.gather_for_metrics((predictions, targets))\n",
       "   194                                                     # num_elements += all_predictions.shape[0]\n",
       "   195                                                     # val_loss += loss_function(all_predictions, all_targets).item()\n",
       "   196                                                     val_loss += loss_function(predictions, targets).item()\n",
       "   197                                         \n",
       "   198                                                 val_loss /= len(eval_dataloader)\n",
       "   199                                                 val_rmse = np.round(np.sqrt(val_loss), 5)\n",
       "   200                                                 # accelerator.print(f\"  Validation RMSE: {val_rmse:>8f}\")\n",
       "   201                                                 print(f\"  Validation RMSE: {val_rmse:>8f}\")\n",
       "   202                                                 # check validation score, if improved then save model\n",
       "   203                                                 if min_valid_metric > val_rmse:\n",
       "   204                                                     # accelerator.print(f\"  Validation RMSE Decreased({min_valid_metric:.6f}--->{val_rmse:.6f})\")\n",
       "   205                                                     print(f\"  Validation RMSE Decreased({min_valid_metric:.6f}--->{val_rmse:.6f})\")\n",
       "   206                                                     min_valid_metric = val_rmse\n",
       "   207                                         \n",
       "   208                                                     # Saving Model State Dict\n",
       "   209                                                     # unwrap_model = accelerator.unwrap_model(model)  # Unwrap the Accelerator model\n",
       "   210                                                     # accelerator.save(unwrap_model.state_dict(), best_model_path)\n",
       "   211                                                     # accelerator.print(f\"  Best Model file path: {best_model_path}\")\n",
       "   212                                                     torch.save(model.state_dict(), best_model_path)\n",
       "   213                                                     print(f\"  Best Model file path: {best_model_path}\")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mprun -f training_loop\n",
    "training_loop(*train_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
