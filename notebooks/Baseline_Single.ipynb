{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3f6333-379a-44cc-99dc-002087eb3868",
   "metadata": {},
   "source": [
    "# Train Baseline UNET model with Single Satellite Image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce072fa3-d560-4f50-acf5-6cd39a44aeda",
   "metadata": {},
   "source": [
    "!pip install --upgrade rasterio s3fs"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e78446f-1221-432d-aa24-87bde670c042",
   "metadata": {},
   "source": [
    "# Install the local package\n",
    "!pip install -e /notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fce2ba2-67a9-4755-960b-bde2ae45ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e39f8b3-cfea-4b51-b412-31daf448f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb7f383b-3d3e-43c8-b976-4b221673cc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from biomasstry.datasets import Sentinel2\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "794a57f0-3ef5-47c3-b759-9f19bd9ac4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.multiprocessing.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "823fc85f-5f83-422d-882c-d012b248bebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_options = {'anon': True}\n",
    "# s3_fs = s3fs.S3FileSystem(**storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd1e69a0-0505-47f5-9d31-f54873e5593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen2dataset = Sentinel2()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d29f029-dba2-46bc-9807-6421e6bc6e81",
   "metadata": {},
   "source": [
    "start = time()\n",
    "idx_data = sen2dataset[0]\n",
    "end = time()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8392c88f-fe22-469e-8576-a1a95a9c94e4",
   "metadata": {},
   "source": [
    "print(f\"Read time: {end - start}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "381ec290-415e-4021-8660-208b15d8bea0",
   "metadata": {},
   "source": [
    "sen2loader = DataLoader(sen2dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a68f1837-8f8d-4652-94de-392a90a27a8c",
   "metadata": {},
   "source": [
    "start = time()\n",
    "batch_data = next(iter(sen2loader))\n",
    "end = time()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5726cd93-d922-44e9-8c2a-15335e60c38d",
   "metadata": {},
   "source": [
    "print(f\"Read time: {end - start}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fca0c4b5-b7a6-4847-a405-cbe078d3556e",
   "metadata": {},
   "source": [
    "batch_data['image'].shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23c9d605-b6a8-46f8-a17a-261283c74052",
   "metadata": {},
   "source": [
    "sen2loader_mp = DataLoader(sen2dataset, batch_size=32, num_workers=8, multiprocessing_context='spawn')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f223e4f-0c25-41ad-a6ed-2b14a6d9c94d",
   "metadata": {},
   "source": [
    "start = time()\n",
    "batch_data_mp = next(iter(sen2loader_mp))\n",
    "end = time()\n",
    "print(f\"Read time: {end - start}\")\n",
    "batch_data_mp['image'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474e3d9d-9f10-4b3a-b7c7-a16e6cc37208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330d2718-3d23-442f-a5ac-4aafec0b61ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6951 Val. samples: 1738\n"
     ]
    }
   ],
   "source": [
    "# split\n",
    "from torch.utils.data import random_split\n",
    "torch.manual_seed(0)\n",
    "train_frac = 0.8\n",
    "train_samples = round(train_frac * len(sen2dataset))\n",
    "val_samples = round((1 - train_frac) * len(sen2dataset))\n",
    "\n",
    "train_dataset, val_dataset = random_split(sen2dataset, [train_samples, val_samples])\n",
    "print(f\"Train samples: {len(train_dataset)} \"\n",
    "      f\"Val. samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a65948e-fac1-436f-96eb-54fb02cd3904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "batch_size = 64  # Note: training speed is sensitive to memory usage\n",
    "                 # set this as high as you can without significantly slowing down training time \n",
    "num_workers = 4\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=True\n",
    "                             )\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=num_workers,\n",
    "                            pin_memory=True\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034015a2-c495-411c-a994-672f52657114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcfa926c-e19b-4a98-a02d-8aa93a8cac5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# input channels: 10\n",
      "Image shape: torch.Size([10, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "img_data = train_dataset[0]['image']\n",
    "in_channels = img_data.shape[0]\n",
    "print(f'# input channels: {in_channels}')\n",
    "print(f\"Image shape: {img_data.shape}\")\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",\n",
    "    encoder_weights=None, # 'imagenet' weights don't seem to help so start clean \n",
    "    in_channels=in_channels,                 \n",
    "    classes=1,                     \n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e887c76-2c8e-4818-bee9-7b95853ff1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "import torch.nn as nn\n",
    "loss_module = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38fd2c67-8fed-4eeb-aa9a-30560fd1b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation Loops\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    train_metrics = []\n",
    "    \n",
    "    print('Training')\n",
    "    for ix, batch in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "        X = batch['image'].to(device)\n",
    "        y = batch['target'].to(device)\n",
    "        \n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_metrics.append(np.round(np.sqrt(loss.item()), 5))\n",
    "            \n",
    "    return train_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ccfa2e9-7ada-4e85-8993-de8e01fc858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_loop(dataloader, model, loss_fn):\n",
    "    num_batches = len(dataloader)\n",
    "    valid_loss = 0\n",
    "    valid_metrics = {}\n",
    "\n",
    "    print('Validation')\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, total=num_batches):\n",
    "            X = batch['image'].to(device)\n",
    "            y = batch['target'].to(device)\n",
    "            \n",
    "            pred = model(X)\n",
    "            valid_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "    valid_loss /= num_batches\n",
    "    valid_rmse = np.round(np.sqrt(valid_loss), 5)\n",
    "    print(f\"Validation Error: \\n RMSE: {valid_rmse:>8f} \\n\")\n",
    "    return valid_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9049085-aefc-495d-88d9-18e70b8feb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, loss_module, optimizer, train_dataloader, val_dataloader, save_path, n_epochs=10):\n",
    "    min_valid_metric = np.inf\n",
    "    train_metrics = []\n",
    "    valid_metrics = []\n",
    "\n",
    "    for ix in range(n_epochs):\n",
    "        print(f\"\\n-------------------------------\\nEpoch {ix+1}\")\n",
    "        train_metrics_epoch = train_loop(train_dataloader, model, loss_module, optimizer)\n",
    "        train_metrics.extend(train_metrics_epoch)\n",
    "        \n",
    "        valid_metrics_epoch = valid_loop(val_dataloader, model, loss_module)\n",
    "        valid_metrics.append((len(train_metrics), valid_metrics_epoch))\n",
    "\n",
    "        # check validation score, if improved then save model\n",
    "        if min_valid_metric > valid_metrics_epoch:\n",
    "            print(f'Validation RMSE Decreased({min_valid_metric:.6f}--->{valid_metrics_epoch:.6f}) \\t Saving The Model')\n",
    "            min_valid_metric = valid_metrics_epoch\n",
    "\n",
    "            # Saving State Dict\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "    print(\"Done!\")\n",
    "    train_metrics_zipped = list(zip(np.arange(0, len(train_metrics)), train_metrics))\n",
    "    \n",
    "    return {'training': train_metrics_zipped, 'validation': valid_metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8265e95d-05f5-4e89-a5c4-173746afc21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir_saved_models = \"../artifacts\"\n",
    "save_file = 'UNET_resnet50_20band_batch32_AGBMLinear_AllTrain_5epoch_01DEC.pt'\n",
    "save_path = os.path.join(dir_saved_models, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb8a4ab3-ffec-4ecb-be6e-1760731e97a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------------------\n",
      "Epoch 1\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d729e63958415b8ad2503d08f8e926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f8d89ed76448af8c5296b5279ccd92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Error: \n",
      " RMSE: 52.403400 \n",
      "\n",
      "Validation RMSE Decreased(inf--->52.403400) \t Saving The Model\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Kickoff training\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "n_epochs = 1\n",
    "start = time()\n",
    "metrics = run_training(model=model,\n",
    "                       loss_module=loss_module,\n",
    "                       optimizer=optimizer,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       val_dataloader=val_dataloader,\n",
    "                       save_path=save_path,\n",
    "                       n_epochs=n_epochs)\n",
    "end = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af662c44-65b9-4213-8fd6-55b2dd69eff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for one epoch: 145.1281087398529\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time for one epoch: {end - start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
