{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune UTAE model to predict AGBM from Temporal Sentinel-2 Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from biomasstry.datasets import TemporalSentinel2Dataset, TemporalSentinel1Dataset\n",
    "from biomasstry.models import TemporalSentinelModel, UTAE\n",
    "from biomasstry.models.unet_tae import ConvBlock\n",
    "# from biomasstry.models.utils import run_training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadata\n",
    "metadata_file = \"/notebooks/data/metadata_parquet/features_metadata_slim.parquet\"\n",
    "metadata_df = pd.read_parquet(metadata_file)\n",
    "chip_ids = metadata_df[metadata_df.split == \"train\"].chip_id.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_perm = np.random.permutation(len(chip_ids))\n",
    "cut = int(0.8 * len(chip_ids))\n",
    "train_split = random_perm[:cut]\n",
    "eval_split = random_perm[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "S3_DIRECT = False  # Access S3 directly or as a mounted data source\n",
    "if S3_DIRECT:\n",
    "    data_url=\"s3://drivendata-competition-biomassters-public-us\"\n",
    "else:\n",
    "    data_url = \"\"\n",
    "\n",
    "datasets = [\"Sentinel-1A\",  # Sentinel-1 Ascending only\n",
    "            \"Sentinel-1D\",  # Sentinel-1 Descending only\n",
    "            \"Sentinel-2all\"]\n",
    "\n",
    "dataset = datasets[2]\n",
    "if dataset == \"Sentinel-1A\":\n",
    "    ds = TemporalSentinel1Dataset(data_url=data_url, bands=[\"VVA\", \"VHA\"])\n",
    "    input_nc = 2\n",
    "    n_tsamples = 6\n",
    "elif dataset == \"Sentinel-1A\":\n",
    "    ds = TemporalSentinel1Dataset(data_url=data_url, bands=[\"VVD\", \"VHD\"])\n",
    "    input_nc = 2\n",
    "    n_tsamples = 6\n",
    "elif dataset == \"Sentinel-2all\":\n",
    "    # ds = TemporalSentinel2Dataset(data_url=data_url)\n",
    "    train_set = TemporalSentinel2Dataset([chip_ids[i] for i in train_split],\n",
    "                                        data_url=data_url)\n",
    "    eval_set = TemporalSentinel2Dataset([chip_ids[i] for i in eval_split],\n",
    "                                       data_url=data_url)\n",
    "    input_nc = 10\n",
    "    n_tsamples = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 6951 Val. samples: 1738\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train samples: {len(train_set)} \"\n",
    "      f\"Val. samples: {len(eval_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "artifacts_dir = \"/notebooks/artifacts\"\n",
    "pretrained_weights_path = artifacts_dir + \"/pretrained_utae/f1model.pth.tar\"\n",
    "model = UTAE(input_nc, out_conv=[32, 20])  # .to(accelerator.device)\n",
    "saved_dict = torch.load(pretrained_weights_path, map_location=device)\n",
    "model.load_state_dict(saved_dict[\"state_dict\"])\n",
    "model.out_conv = ConvBlock([32, 32, 1], padding_mode=\"reflect\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss(reduction='mean')  # .to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file path: /notebooks/artifacts/20230118_UTAE-pretrainedF1_Sentinel-2all_B8_E10.pt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "model_name = \"UTAE-pretrainedF1\"\n",
    "nb_epochs = 10\n",
    "date = \"20230118\"\n",
    "save_path = artifacts_dir + (f\"/{date}_{model_name}_{dataset}_B\"\n",
    "    f\"{batch_size}\"\n",
    "    f\"_E{nb_epochs}.pt\")\n",
    "print(f\"Model file path: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "num_workers = 6\n",
    "train_dataloader = DataLoader(train_set,\n",
    "                      batch_size=batch_size,\n",
    "                      shuffle=True,\n",
    "                      pin_memory=True,\n",
    "                      num_workers=num_workers)\n",
    "eval_dataloader = DataLoader(eval_set,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    pin_memory=True,\n",
    "                    num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6e011ace394e3792c80db42021e2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e314c88a240c48ac981eb708ede7335e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%mprun\n",
    "num_batches = len(eval_dataloader)\n",
    "train_metrics = []\n",
    "val_metrics = []\n",
    "min_valid_metric = np.inf\n",
    "\n",
    "for i in tqdm(range(nb_epochs)):\n",
    "    train_metrics_epoch = []\n",
    "    epoch_start = time()\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        inputs, targets, _ = batch\n",
    "        outputs = model(inputs.to(device))\n",
    "        loss = loss_function(outputs, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_metrics_epoch.append(np.round(np.sqrt(loss.item()), 5))\n",
    "\n",
    "    epoch_end = time()\n",
    "    print(f\"Epoch training time: {epoch_end - epoch_start}\")\n",
    "    \n",
    "    # Saving State Dict after each epoch\n",
    "    torch.save(model.state_dict(), save_path[:-3] + \"_Ep{i}.pt\")\n",
    "    \n",
    "    # Validation Loop\n",
    "    val_loss = 0.0\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        inputs, targets, _ = batch\n",
    "        with torch.no_grad():\n",
    "            predictions = model(inputs.to(device))\n",
    "        val_loss += loss_function(predictions, targets.to(device)).item()\n",
    "\n",
    "    val_loss /= num_batches\n",
    "    val_rmse = np.round(np.sqrt(val_loss), 5)\n",
    "    print(f\"Validation Error: \\n RMSE: {val_rmse:>8f} \\n\")\n",
    "    train_metrics.extend(train_metrics_epoch)\n",
    "    val_metrics.append((len(train_metrics), val_rmse))\n",
    "    # check validation score, if improved then save model\n",
    "    if min_valid_metric > val_rmse:\n",
    "        print(f'Validation RMSE Decreased({min_valid_metric:.6f}--->{val_rmse:.6f}) \\t Saving The Model')\n",
    "        min_valid_metric = val_rmse\n",
    "\n",
    "        # Saving State Dict\n",
    "        torch.save(model.state_dict(), save_path[:-3] + \"_BEST.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "##### Save the metrics to a file\n",
    "train_metrics_zipped = list(zip(np.arange(0, len(train_metrics)), train_metrics))\n",
    "metrics = {'training': train_metrics_zipped, 'validation': val_metrics}\n",
    "train_metrics_df = pd.DataFrame(metrics['training'], columns=[\"step\", \"score\"])\n",
    "val_metrics_df = pd.DataFrame(metrics[\"validation\"], columns=[\"step\", \"score\"])\n",
    "train_metrics_df.to_csv(artifacts_dir + \"/train_metrics.csv\")\n",
    "val_metrics_df.to_csv(artifacts_dir + \"/val_metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
